{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac53433e",
   "metadata": {},
   "source": [
    "Anomaly-Detection-LSTM-GRU\n",
    "==============================\n",
    "\n",
    "Some experiments with LSTMs and GRUs for anomaly detection use cases on publicly available dataset(s).\n",
    "We demonstrate the use of a Seq2Seq Autoencoder for detecting anomalies in multivariate time series.\n",
    "The data we use was create in a scaled down water treatment system for research purposes. \n",
    "![Water treatment plant](./references/plant_pic.png)\n",
    "The corresponding paper can be found [here](https://link.springer.com/chapter/10.1007%2F978-3-319-71368-7_8).\n",
    "Some helpful dataset description can be found [here](https://itrust.sutd.edu.sg/itrust-labs_datasets/dataset_info/)\n",
    "\n",
    "Run the experiment\n",
    "---\n",
    "Setup\n",
    "- clone this repo\n",
    "- download the whole 2015 folder from [this link](https://drive.google.com/drive/folders/1ABZKdclka3e2NXBSxS9z2YF59p7g2Y5I)\n",
    "- copy all the zip archives the raw data directory\n",
    "- navigate to the project root and run:\n",
    "```shell\n",
    "unzip './data/raw/*.zip' -d ./data/interim \n",
    "mv ./data/interim/SWaT.A1\\ _\\ A2_Dec\\ 2015/*.pdf ./references\n",
    "mv ./data/interim/SWaT.A1 _ A2_Dec 2015/* ./data/interim/\n",
    "rm -r ./data/interim/SWaT.A1 _ A2_Dec 2015\n",
    "```\n",
    "- set up virtual environment (conda in my case) and install the dependencies. From the root of this project, run:\n",
    "```shell\n",
    "conda create -n ano \n",
    "conda activate ano\n",
    "pip isntal -e .\n",
    "```\n",
    "- Optional: if you want to have reproduce exactly the environment I was working in, run the following with the 'ano' environment activated (all my development tools are also listed):\n",
    "```shell\n",
    "pip insatll -r requirements.txt\n",
    "```\n",
    "- Now run the following command to transform the excel files into a h5 data store \n",
    "```shell\n",
    "python ./anodeclstmgru/data/create_h5_file.py\n",
    "```\n",
    "- From here you should be able to run the data exploration notebooks ('notebooks/01-Exploration.ipynb' and 'notebooks/02-Time_series_exploration.ipynb')\n",
    "- Now run the following to run the data preparation script:\n",
    "```shell\n",
    "python anodeclstmgru/data/data_preparation.py\n",
    "```\n",
    "- To kick off model training processes, you  can modify and use the scripts stored in the 'utils' directory. E.g.:\n",
    "```shell\n",
    "./utils/run_training_gpu.sh\n",
    "```\n",
    "- I you like to see the training process in a tensorboard, run:\n",
    "```shell\n",
    "tensorbaord --logdir lightning_logs\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Project Organization\n",
    "------------\n",
    "\n",
    ".\n",
    "├── README.md\n",
    "├── anodeclstmgru\n",
    "│   ├── __init__.py\n",
    "│   ├── constants.py\n",
    "│   ├── data\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── create_h5_file.py\n",
    "│   │   ├── data_module.py\n",
    "│   │   ├── data_preparation.py\n",
    "│   │   └── dataset.py\n",
    "│   ├── features\n",
    "│   │   └── __init__.py\n",
    "│   ├── models\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── lit_module.py\n",
    "│   │   └── train_model.py\n",
    "│   └── visualization\n",
    "│       └── __init__.py\n",
    "├── data\n",
    "│   ├── external\n",
    "│   ├── interim\n",
    "│   │   ├── List_of_attacks_Final.xlsx\n",
    "│   │   ├── Network\n",
    "│   │   │   ├── 2015-12-22_034215_69.log.part01_sorted.csv\n",
    "│   │   │   ├── 2015-12-22_034215_69.log.part02_sorted.csv\n",
    "│   │   │   ├── 2015-12-22_034215_69.log.part03_sorted.csv\n",
    "│   │   │   └── ... \n",
    "│   │   ├── Physical\n",
    "│   │   │   ├── SWaT_Dataset_Attack_v0.xlsx\n",
    "│   │   │   ├── SWaT_Dataset_Normal_v0.xlsx\n",
    "│   │   │   └── SWaT_Dataset_Normal_v1.xlsx\n",
    "│   │   └── data_store.h5\n",
    "│   ├── processed\n",
    "│   │   └── data_store.h5\n",
    "│   └── raw\n",
    "├── lightning_logs\n",
    "├── models\n",
    "├── notebooks\n",
    "│   ├── 01-Exploration.ipynb\n",
    "│   ├── 02-Time_series_exploration.ipynb\n",
    "│   └── 03-Model_output_exploration.ipynb\n",
    "├── references\n",
    "│   ├── A Dataset to Support Research in the Design of Secure Water Treatment Systems.pdf\n",
    "│   ├── List_of_attacks_Final.pdf\n",
    "│   └── plant_pic.png\n",
    "├── reports\n",
    "│   └── figures\n",
    "├── requirements.txt\n",
    "├── setup.py\n",
    "└── utils\n",
    "    ├── run_training_gpu.sh\n",
    "    └── run_training_no_gpu.sh\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ├── LICENSE\n",
    "    ├── README.md          <- This file =)\n",
    "    ├── data\n",
    "    │   ├── interim        <- Intermediate data that has been transformed.\n",
    "    │   ├── processed      <- The final, canonical data sets for modeling.\n",
    "    │   └── raw            <- The original, immutable data dump.\n",
    "    │\n",
    "    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "    │                         the creator's initials, and a short `-` delimited description, e.g.\n",
    "    │                         `1.0-jqp-initial-data-exploration`.\n",
    "    │\n",
    "    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n",
    "    │\n",
    "    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "    │   └── figures        <- Generated graphics and figures to be used in reporting\n",
    "    │\n",
    "    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n",
    "    │                         generated with `pip freeze > requirements.txt`\n",
    "    │\n",
    "    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n",
    "    ├── anodeclstmgru      <- Source code for use in this project.\n",
    "    │   ├── __init__.py    <- Makes src a Python module\n",
    "    │   │\n",
    "    │   ├── data           <- Scripts to download or generate data\n",
    "    │   │\n",
    "    │   ├── features       <- Scripts to turn raw data into features for modeling\n",
    "    │   │\n",
    "    │   ├── models         <- Scripts to train models and then use trained models to make\n",
    "    │   │                     predictions\n",
    "    │   │\n",
    "    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations\n",
    "    │\n",
    "    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io\n",
    "\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61e50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
